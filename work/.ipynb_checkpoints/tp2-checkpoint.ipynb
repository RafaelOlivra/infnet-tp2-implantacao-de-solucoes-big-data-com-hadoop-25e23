{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1996951a",
   "metadata": {},
   "source": [
    "# Projeto Big Data - Demonstra√ß√£o com Hadoop, Hive, Pig, Spark, HBase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889c051d",
   "metadata": {},
   "source": [
    "Este notebook demonstra o uso das principais ferramentas do ecossistema Hadoop para um projeto Big Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9c6ae",
   "metadata": {},
   "source": [
    "## üî• Apache Spark - Leitura de JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataProject\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Leitura de arquivo JSON\n",
    "df_json = spark.read.json(\"config.json\")\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b610f4",
   "metadata": {},
   "source": [
    "## üêù Apache Hive - Cria√ß√£o de tabela e inser√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se o Hive estiver integrado com Spark:\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS produtos (\n",
    "    id INT,\n",
    "    nome STRING,\n",
    "    preco FLOAT\n",
    ")\n",
    "STORED AS PARQUET\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a3458",
   "metadata": {},
   "source": [
    "## üêò Hive - Consulta com filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM produtos WHERE preco > 100\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f038e5",
   "metadata": {},
   "source": [
    "## üê∑ Apache Pig - Script b√°sico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80acb4f3",
   "metadata": {},
   "source": [
    "\n",
    "```pig\n",
    "vendas = LOAD 'vendas.csv' USING PigStorage(',') AS (id:int, produto:chararray, valor:float);\n",
    "filtro = FILTER vendas BY valor > 500;\n",
    "DUMP filtro;\n",
    "```\n",
    "Salvar isso em um arquivo `.pig` e executar com:\n",
    "```bash\n",
    "pig script.pig\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc97e07",
   "metadata": {},
   "source": [
    "## üßä Hadoop HDFS - Comandos √∫teis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /input\n",
    "!hdfs dfs -put dados.csv /input\n",
    "!hdfs dfs -ls /input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53d4b84",
   "metadata": {},
   "source": [
    "## üíæ Apache Hive - Carregamento de dados CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0796308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vendas (\n",
    "    id INT,\n",
    "    produto STRING,\n",
    "    valor FLOAT\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "\"\"\")\n",
    "\n",
    "# Se o arquivo estiver no HDFS\n",
    "spark.sql(\"LOAD DATA INPATH '/input/vendas.csv' INTO TABLE vendas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e51e5",
   "metadata": {},
   "source": [
    "## üßÆ Spark - Total de vendas por produto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vendas = spark.read.csv(\"vendas.csv\", header=True, inferSchema=True)\n",
    "df_vendas.groupBy(\"produto\").sum(\"valor\").withColumnRenamed(\"sum(valor)\", \"total_vendas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4297c8db",
   "metadata": {},
   "source": [
    "## üìä Hive - Tabela resumo de vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a5854",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vendas_resumo AS\n",
    "SELECT produto, SUM(valor) AS total_vendas\n",
    "FROM vendas\n",
    "GROUP BY produto\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c204b",
   "metadata": {},
   "source": [
    "## üê∑ Pig - Agrupamento de logs por URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceca9931",
   "metadata": {},
   "source": [
    "\n",
    "```pig\n",
    "logs = LOAD 'logs.csv' USING PigStorage(',') AS (data:chararray, ip:chararray, url:chararray);\n",
    "grouped = GROUP logs BY url;\n",
    "counts = FOREACH grouped GENERATE group AS url, COUNT(logs) AS acessos;\n",
    "DUMP counts;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b3f79",
   "metadata": {},
   "source": [
    "## üß± HBase - Cria√ß√£o de tabela"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b33d7",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "hbase shell\n",
    "> create 'clientes', 'info'\n",
    "> put 'clientes', '1001', 'info:nome', 'Alice'\n",
    "> put 'clientes', '1001', 'info:email', 'alice@email.com'\n",
    "> scan 'clientes'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca9636a",
   "metadata": {},
   "source": [
    "## üê∑ Pig + HBase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdad7d8",
   "metadata": {},
   "source": [
    "\n",
    "```pig\n",
    "REGISTER /usr/lib/pig/piggybank.jar\n",
    "DEFINE HBaseStorage org.apache.pig.backend.hadoop.hbase.HBaseStorage;\n",
    "\n",
    "clientes = LOAD 'hbase://clientes'\n",
    "    USING HBaseStorage('info:nome info:email')\n",
    "    AS (nome:chararray, email:chararray);\n",
    "\n",
    "DUMP clientes;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f367953",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è PySpark - Importar dados do SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9604f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clientes = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:sqlserver://servidor:1433;databaseName=meubanco\") \\\n",
    "    .option(\"dbtable\", \"clientes\") \\\n",
    "    .option(\"user\", \"usuario\") \\\n",
    "    .option(\"password\", \"senha\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()\n",
    "\n",
    "df_clientes.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}